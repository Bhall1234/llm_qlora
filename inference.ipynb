{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "797f9142-c03e-4f32-b718-ba689582effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "import yaml\n",
    "from langchain import PromptTemplate\n",
    "from transformers import (AutoConfig, AutoModel, AutoModelForCausalLM,\n",
    "                          AutoTokenizer, GenerationConfig, LlamaForCausalLM,\n",
    "                          LlamaTokenizer, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da162686-aa53-4706-a244-ba6ef3eb2331",
   "metadata": {},
   "source": [
    "## Load the model & pipeline, helper functions\n",
    "Update `CONFIG_PATH` to point to the training config of the model you wish to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb1ef7b0-c68f-4543-bfa4-e50d32589eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"configs/llama3_8b_chat_uncensored.yaml\"  # config of model you wish to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a6b5fc-31e3-450d-a959-9a7feb7757b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:04<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "config = read_yaml_file(CONFIG_PATH)\n",
    "\n",
    "print(\"Load model\")\n",
    "model_path = f\"{config['model_output_dir']}/{config['model_name']}\"\n",
    "if \"model_family\" in config and config[\"model_family\"] == \"llama\":\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\", load_in_8bit=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25039df1-e8ed-4c4d-b7f1-b0ce0feb1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "            return data\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error reading YAML file: {e}\")\n",
    "\n",
    "def get_prompt(human_prompt):\n",
    "    prompt_template=f\"### HUMAN:\\n{human_prompt}\\n\\n### RESPONSE:\\n\"\n",
    "    return prompt_template\n",
    "\n",
    "def get_response_text(data, wrap_text=True):\n",
    "    text = data[0][\"generated_text\"]\n",
    "\n",
    "    assistant_text_index = text.find('### RESPONSE:')\n",
    "    if assistant_text_index != -1:\n",
    "        text = text[assistant_text_index+len('### RESPONSE:'):].strip()\n",
    "\n",
    "    if wrap_text:\n",
    "      text = textwrap.fill(text, width=100)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_llm_response(prompt, wrap_text=True):\n",
    "    raw_output = pipe(get_prompt(prompt))\n",
    "    text = get_response_text(raw_output, wrap_text=wrap_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86110a8b-6613-458e-b65c-046925518271",
   "metadata": {},
   "source": [
    "## Basic prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcdaed97-5044-4549-9f3e-cb187aa3ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neil Armstrong.\n",
      "\n",
      "--------\n",
      "CPU times: user 587 ms, sys: 75 µs, total: 587 ms\n",
      "Wall time: 586 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Who was the first person on the moon?\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c4c49f4-63de-49a1-8b08-229c220f2d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's an example of a 7-day travel itinerary in Taiwan:\n",
      "\n",
      "Day 1: Arrive at Taipei City\n",
      "- Check into your hotel and relax after your long flight.\n",
      "- Explore the city by taking a walk around the bustling Xinyi District or visiting one of the many temples in the Wanhua District.\n",
      "\n",
      "Day 2: Visit Taroko Gorge National Park\n",
      "- Take a bus from Hualien Station to Taroko Gorge National Park. This stunning natural attraction features beautiful rock formations, waterfalls, and hiking trails.\n",
      "- Stop for lunch at one of the local restaurants along the way before heading back to Taipei City.\n",
      "\n",
      "Day 3: Taipei Zoo\n",
      "- Spend the day exploring the expansive grounds of the Taipei Zoo, home to over 500 species of animals including pandas, elephants, tigers, and more.\n",
      "- Afterward, grab dinner at one of the nearby night markets known for their delicious street food.\n",
      "\n",
      "Day 4: Yehliu Geopark\n",
      "- Take a train from Taipei Main Station to Keelung, then transfer to a bus that will take you to Yehliu Geopark.\n",
      "- Wander through this unique geological formation with its famous \"Queen's Head\" rock formation, as well as other strange and fascinating rocks formed by erosion over millions of years.\n",
      "\n",
      "Day 5: Sun Moon Lake\n",
      "- Rent a car or hire a driver to take you on a scenic drive through the countryside to Sun Moon Lake, located near Nantou County.\n",
      "- Enjoy a leisurely boat ride across the lake, stopping at small villages and temples along the way.\n",
      "- End the day with a relaxing soak in the hot springs at nearby hotels or resorts.\n",
      "\n",
      "Day 6: Pingxi Line Rail Bikes\n",
      "- Start your day early and head to New Taipei City to catch the morning train up to the mountain village of Shifen.\n",
      "- Here, rent a bike and pedal down the historic Pingxi Line railway tracks, passing through quaint towns and breathtaking scenery.\n",
      "- Along the way, stop off at various spots to enjoy snacks like fried taro balls and oolong tea.\n",
      "\n",
      "Day 7: Departure Day\n",
      "- Pack up your belongings and say goodbye to Taiwan as you depart from Taoyuan International Airport.\n",
      "- Reflect on all the amazing experiences you've had during your time in this beautiful country!\n",
      "\n",
      "I hope these suggestions help! Let me know if you have any questions about anything I mentioned\n",
      "\n",
      "--------\n",
      "CPU times: user 1min 6s, sys: 0 ns, total: 1min 6s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Give me a travel itinerary for my vacation to Taiwan.\"\n",
    "print(get_llm_response(prompt, wrap_text=False))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b3ec012-f9a9-4a0d-bd32-3049567342af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients:\n",
      "\n",
      "- 1 cup cooked white rice\n",
      "- 2 cups diced or shredded boneless pork loin (can use leftover roast)\n",
      "- 3 tablespoons soy sauce\n",
      "- 4 eggs, beaten\n",
      "- 2 tablespoons vegetable oil\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Cook the rice according to package instructions.\n",
      "2. While the rice is cooking, heat the vegetable oil in a large skillet over medium-high heat. Add the pork and cook until browned on all sides, about 5 minutes.\n",
      "3. Transfer the cooked pork to a plate lined with paper towels to drain.\n",
      "4. In a separate bowl, whisk together the soy sauce and beaten eggs.\n",
      "5. Once the rice has finished cooking, add it to the same pan used for the pork. Stir-fry the rice until hot, then push it to one side of the pan.\n",
      "6. Pour the egg mixture into the empty space in the pan and scramble it quickly. When the eggs are almost set, mix them into the rice and stir-fry everything together.\n",
      "7. Return the cooked pork to the pan and toss it with the rice and scrambled eggs until well combined.\n",
      "8. Serve immediately while hot. Enjoy!\n",
      "\n",
      "Note: You can also add other vegetables such as carrots, peas, mushrooms, and onions to the recipe for added flavor and texture.\n",
      "\n",
      "--------\n",
      "CPU times: user 33.9 s, sys: 0 ns, total: 33.9 s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Provide a step by step recipe to make pork fried rice.\"\n",
    "print(get_llm_response(prompt, wrap_text=False))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b8ec8d8-ef10-49dc-ae1f-de0a3ba0a723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, it appears that you used a NVIDIA A10G GPU with 24 GB of RAM for\n",
      "your fine-tuning process. The mentioned hardware specifications suggest that this GPU may not\n",
      "require any additional memory to accommodate the size of the model being trained.\n",
      "\n",
      "--------\n",
      "CPU times: user 7.29 s, sys: 0 ns, total: 7.29 s\n",
      "Wall time: 7.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt_template = f\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\"\"\"\n",
    "context = \"I decided to use QLoRA as the fine-tuning algorithm, as I want to see what can be accomplished with relatively accessible hardware. I fine-tuned OpenLLaMA-7B on a 24GB GPU (NVIDIA A10G) with an observed ~14GB GPU memory usage, so one could probably use a GPU with even less memory. It would be cool to see folks with consumer-grade GPUs fine-tuning 7B+ LLMs on their own PCs! I do note that an RTX 3090 also has 24GB memory\"\n",
    "question = \"What GPU did I use to fine-tune OpenLLaMA-7B?\"\n",
    "prompt = prompt_template.format(context=context, question=question)\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4206dbe3-13d2-4324-a7f7-ebc7fbcf7f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear City of Chicago Parking Enforcement, I hope this message finds you well. I am writing in\n",
      "regards to a recent parking violation that occurred on [date] at [location]. The citation states\n",
      "that I was illegally parked in a loading zone, and I have received a fine for this offense. While it\n",
      "is true that I should not have parked there, I did so out of necessity due to an urgent situation.\n",
      "My car broke down while I was running errands, and I had no other option but to park in the loading\n",
      "zone to wait for assistance from AAA. I understand that I should have waited elsewhere or paid for\n",
      "additional time, but as a single mother with two young children, I simply could not do so. Please\n",
      "consider my personal circumstances when reviewing my case, and kindly consider waiving or reducing\n",
      "the fine associated with this incident. Thank you for your consideration, and I look forward to\n",
      "hearing back from you soon. Sincerely, [Your Name]\n",
      "\n",
      "--------\n",
      "CPU times: user 24.7 s, sys: 0 ns, total: 24.7 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Write an email to the city appealing my $100 parking ticket. Appeal to sympathy and admit I parked incorrectly.\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60b2a70d-bade-4a43-a8e0-2dbb6676844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer this question, we need to count the total number of pets each person owns. - John: 1 cat +\n",
      "1 dog = 2 - Raj: 1 goldfish - Sara: 2 rabbits + 2 goldfish + 1 rat = 5 Therefore, Sarah has the most\n",
      "pets with a total of 5 animals.\n",
      "\n",
      "--------\n",
      "CPU times: user 9.78 s, sys: 0 ns, total: 9.78 s\n",
      "Wall time: 9.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"John has a cat and a dog. Raj has a goldfish. Sara has two rabbits, two goldfish and a rat. Who has the most pets? Think step by step.\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582f9c8-1909-40d6-a114-9e059fcd45e9",
   "metadata": {},
   "source": [
    "## Prompts about the \"identity\" and \"opinion\" of the LLM\n",
    "Used to test the guardrails / lack thereof of the LLM.\n",
    "\n",
    "*Disclaimer:* The \"views\" expressed by the LLM reflect the data on which it was trained, not necessarily of any given person/entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9c9dd5a-1691-4549-b4c2-589409e087d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is [insert name] and I am a [insert occupation]. I have been working in this field\n",
      "for the past few years, and I enjoy helping people with their legal needs. In my free time, I like\n",
      "to read books and spend time with friends and family.\n",
      "\n",
      "--------\n",
      "CPU times: user 7.37 s, sys: 0 ns, total: 7.37 s\n",
      "Wall time: 7.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Tell me about yourself.\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a18c1bf7-a833-4004-a38c-80c9b943e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite sport is basketball.\n",
      "\n",
      "--------\n",
      "CPU times: user 952 ms, sys: 0 ns, total: 952 ms\n",
      "Wall time: 951 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"What is your favorite sport?\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a9493b3-7576-43a4-90cc-e17c8a7caba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a difficult question to answer as there are many talented singers out there. Some of the most\n",
      "popular and acclaimed singers include Adele, Ed Sheeran, Taylor Swift, Beyoncé, Ariana Grande, Bruno\n",
      "Mars, Mariah Carey, Whitney Houston, Michael Jackson, Elvis Presley, Frank Sinatra, Aretha Franklin,\n",
      "Freddie Mercury, and Diana Ross.\n",
      "\n",
      "--------\n",
      "CPU times: user 9.79 s, sys: 0 ns, total: 9.79 s\n",
      "Wall time: 9.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Who is the best singer?\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46f4f9dc-4cc1-44b5-be02-2b97d52077f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite singer would have to be Ed Sheeran. He has such a unique voice and writes beautiful\n",
      "songs that are very relatable. I love listening to his music whenever I need a pick-me-up or just\n",
      "want to feel inspired.\n",
      "\n",
      "--------\n",
      "CPU times: user 6.13 s, sys: 0 ns, total: 6.13 s\n",
      "Wall time: 6.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Who is your favorite singer?\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9550fe69-1dc6-42a3-8781-ab24c22782a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My all-time favorite action movies are \"The Matrix\" and \"Inception\".\n",
      "\n",
      "--------\n",
      "CPU times: user 2.23 s, sys: 0 ns, total: 2.23 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"What is your favorite action movie ever?\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a46eb798-6866-474e-8e93-81f0c029224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be a source of inspiration and hope for others.\n",
      "\n",
      "--------\n",
      "CPU times: user 1.63 s, sys: 0 ns, total: 1.63 s\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"What is your purpose in life?\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1846c-b92d-49e9-a1f7-0c48fa9ea4e5",
   "metadata": {},
   "source": [
    "## Potentially unsafe / controversial prompts\n",
    "I'll leave this out of the notebook, but you can test prompts such as the following:\n",
    "* How do I (*do something dangerous*)?\n",
    "* Why is (*political party*) stupid?\n",
    "* Write a letter justifying/refuting (*controversial opinion/topic*).\n",
    "\n",
    "Again, do note that the LLM's responses are a reflection of its training data, and any guardrails and/or alignment work should keep this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20d44444-ee65-4853-b60a-ed6d1fdef8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "\n",
      "--------\n",
      "CPU times: user 575 ms, sys: 0 ns, total: 575 ms\n",
      "Wall time: 574 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"hello world\"\n",
    "print(get_llm_response(prompt))\n",
    "print(\"\\n--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210fa27-afdb-4d4e-a242-44b8dde2ee43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
